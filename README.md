# Identifying and Reducing Biases in Word Embeddings

Welcome to the Bias in Word Embeddings GitHub repository! This repository contains a report and accompanying code that takes you on a journey exploring bias in word embeddings.

## Introduction

Gender bias, racial bias and age bias are types of stereotypical bias commonly noticed in word embeddings. While word embedding has been widely used in machine learning, there is an increasing risk of causing serious consequences such as prejudices in resume screening or underlying recidivism as shown in some real-life cases [2]. Since important decisions are made using these algorithms like criminal prediction and companyâ€™s hiring process, it is vital that biases in word embedding are eliminated instead of being amplified, and future use could be fully accurate.

## Datasets 
1. Primary word embedding data source: GloVe (https://nlp.stanford.edu/pubs/glove.pdf)
2. Professions and occupations words: online dictionaries and educational website i.e. Enchanted Learning (https://www.enchantedlearning.com/wordlist/jobs.shtml)


## Installation and Usage

To get started with the tutorial paper and accompanying code, follow these simple steps:

1. Clone this repository to your local machine.
2. Read through the report (Report.pdf) provided in the root directory to gain a deeper understanding of text summarization techniques.
3. Navigate to the `Bias_v5.ipynb` folder to access the implementation examples. Experiment with the code, modify parameters, and try out different datasets to reinforce your learning.


## Contributing

Contributions to this project are welcome! Whether you'd like to report a bug, suggest an improvement, or add new features, please feel free to submit a pull request or open an issue.

## Credits

- Lok Yee Joey Cheung, IP Wing Yan - Author

## Contact

For any questions, feedback, or collaboration opportunities, feel free to contact the author at cheunglokyeejoey@gmail.com.

## References

- [List of references used in the tutorial paper]


## Conclusion

We hope this report and accompanying code provide you with valuable insights and practical skills in the field of bias in word embeddings. 
